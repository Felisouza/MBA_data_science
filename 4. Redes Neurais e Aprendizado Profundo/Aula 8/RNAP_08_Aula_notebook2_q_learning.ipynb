{"cells":[{"cell_type":"markdown","metadata":{"id":"fvaEK6Bn-wwG"},"source":["## MBA em Ciência de Dados\n","# Redes Neurais e Arquiteturas Profundas\n","\n","### <span style=\"color:darkred\">Módulo 8 - Introdução ao Aprendizado por Reforço</span>\n","\n","#### <span style=\"color:darkred\">**Parte 2: Algoritmo de Aprendizado por Reforço \"Value Learning\"**</span>\n","\n","Moacir Antonelli Ponti\n","\n","CeMEAI - ICMC/USP São Carlos\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OhzNkHA40ckG"},"outputs":[],"source":["import gym\n","import numpy as np\n","from IPython.display import clear_output"]},{"cell_type":"markdown","metadata":{"id":"IecwhjS60ckJ"},"source":["### Problema Taxi\n","\n","Relembrando:\n","* Temos 4 localizações relevantes indicadas por 0:R, 1:G, 2:Y e 3:B.\n","* Azul é o passageiro e magenta o destino.\n","* O taxi é amarelo quando livre e verde quando ocupado\n","* Grid contém 25 posições para o táxi, 5 para o passageiro, e 4 destinos\n","\n","**Ações**:<br>\n","0 : mover para norte<br>\n","1 : mover para sul<br>\n","2 : mover para leste<br>\n","3 : mover para oeste<br>\n","4 : pegar passageiro (pickup)<br>\n","5 : deixar passageiro (dropoff)\n","\n","**Observação/estado**:<br>\n","Posições do taxi, passageiro e destino codificada numericamente\n","\n","**Recompensas**:<br>\n","* -1 : por passo<br>\n","* 20 : deixar passageiro<br>\n","* -10: executar \"pegar\" ou \"deixar\" ilegalmente\n","\n","**Término**:\n","* Passageiro é deixado no destino"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZxxXf790ckK","executionInfo":{"status":"ok","timestamp":1698961089430,"user_tz":180,"elapsed":244,"user":{"displayName":"Moacir Antonelli Ponti","userId":"09722981635546271521"}},"outputId":"79db7457-3b92-4bea-e7b0-690c5501ba45"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(500, 6)"]},"metadata":{},"execution_count":16}],"source":["env = gym.make(\"Taxi-v3\")\n","\n","# tabela Q\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","q_table.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"QLrNpKe90ckO","executionInfo":{"status":"ok","timestamp":1698961089944,"user_tz":180,"elapsed":4,"user":{"displayName":"Moacir Antonelli Ponti","userId":"09722981635546271521"}},"outputId":"f20ba098-53d8-476f-f985-cf3ca9b6973f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0.]\n","[0. 0. 0. 0. 0. 0.]\n"]}],"source":["for i in range(301,305):\n","    print(np.round(q_table[i],4))"]},{"cell_type":"markdown","metadata":{"id":"GwbEJfGI0ckP"},"source":["#### Algoritmo de Value learning\n","\n","Tentaremos predizer o valor atual, ajustando com o melhor valor do estado subsequente.\n","\n","Iremos considerar uma taxa de aprendizado $\\alpha$ e um desconto para recompensas futuras $\\gamma$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBLrcOiP0ckQ"},"outputs":[],"source":["# hiperparametros\n","alpha = 0.1 # taxa de aprendizado\n","gamma = 0.5 # desconto de recompensas futuras\n","\n","# historico\n","episodios = []\n","\n","# episodios\n","for t in range(1, 101):\n","    s = env.reset()\n","\n","    epochs, recompensas = 0, 0\n","    fim = False\n","\n","    # episodio atual\n","    while not fim:\n","        # explorar espaco de acao\n","        a = env.action_space.sample()\n","\n","        # realizar acao\n","        s_n, r, fim, info = env.step(a)\n","        # estado subsequente s_n\n","\n","        # salvo o valor atual para (s,a) - Q(s,a)\n","        valor_ant = q_table[s,a]\n","\n","        # verifica proximo valor\n","        prox_max = np.max(q_table[s_n])\n","\n","        # combina com desconto na recompensa futura\n","        novo_valor = (1-alpha)*valor_ant + alpha*(r+gamma*prox_max)\n","        q_table[s,a] = novo_valor\n","\n","        # atualiza estado\n","        s = s_n\n","        epochs += 1\n","\n","    if (t % 250 == 0):\n","        clear_output(wait=True)\n","        print(\"Episódio: \", t)"]},{"cell_type":"markdown","metadata":{"id":"rJAkp9o10ckS"},"source":["Ao consultar a tabela, teremos *valores* diferentes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BgkM81ps0ckT","executionInfo":{"status":"ok","timestamp":1698961158771,"user_tz":180,"elapsed":255,"user":{"displayName":"Moacir Antonelli Ponti","userId":"09722981635546271521"}},"outputId":"e475fe98-f7b3-4350-dbf7-349ead9d0512"},"outputs":[{"output_type":"stream","name":"stdout","text":["[-1.1404 -1.059  -0.7172 -0.9744 -8.1059 -7.8449]\n","[-0.6963 -0.7288 -0.5557 -0.371  -3.4485 -4.149 ]\n","[-0.8611 -0.6747 -0.5985 -0.8149 -3.4815 -6.2218]\n","[-0.4231 -0.5191 -0.195  -0.6046 -5.7218 -3.4532]\n"]}],"source":["for i in range(301,305):\n","    print(np.round(q_table[i],4))"]},{"cell_type":"markdown","metadata":{"id":"kpqp5sBt0ckV"},"source":["### Avaliando a performance do agente\n","\n","Após obter a função Q, armazenada na tabela, agora podemos avaliar a performance do agente"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"-CyQ-7e00ckX","executionInfo":{"status":"ok","timestamp":1698960975008,"user_tz":180,"elapsed":262,"user":{"displayName":"Moacir Antonelli Ponti","userId":"09722981635546271521"}},"outputId":"8806ac67-eb78-467f-f83d-6a355b6c68a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Média de recompensas totais: 8.42\n","Média de passos por episódio: 12.58\n"]}],"source":["n_episodios_teste = 50\n","total_epochs = 0\n","total_recs = 0\n","\n","for i in range(n_episodios_teste):\n","    s = env.reset()\n","    epochs, rec_total_i = 0,0\n","    fim = False\n","    while not fim:\n","        a = np.argmax(q_table[s])\n","        s, r, fim, info = env.step(a)\n","        epochs += 1\n","        rec_total_i += r\n","\n","    total_epochs += epochs\n","    total_recs += rec_total_i\n","\n","print(\"Média de recompensas totais: %.2f\" % (total_recs/n_episodios_teste))\n","print(\"Média de passos por episódio: %.2f\" % (total_epochs/n_episodios_teste))"]},{"cell_type":"markdown","metadata":{"id":"7m27qryH0ckY"},"source":["#### Executando uma vez para visualizar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-jhtbri0ckY"},"outputs":[],"source":["from time import sleep\n","\n","def animacao_episodio(frames):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'])\n","        print(\"t: \", (i + 1))\n","        print(\"Estado: \", frame['state'])\n","        print(\"Ação: \", frame['action'])\n","        print(\"Recompensa: \", frame['reward'])\n","        sleep(.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":560},"id":"TMJoeWSg0ckZ","executionInfo":{"status":"error","timestamp":1698961187717,"user_tz":180,"elapsed":11426,"user":{"displayName":"Moacir Antonelli Ponti","userId":"09722981635546271521"}},"outputId":"5c63521d-3ba4-4073-da32-b81bf33bc03c"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+\n","|R: | : :\u001b[35m\u001b[43mG\u001b[0m\u001b[0m|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|Y| : |\u001b[34;1mB\u001b[0m: |\n","+---------+\n","  (North)\n","\n","t:  23\n","Estado:  93\n","Ação:  1\n","Recompensa:  -1\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-ef1cdc1285ec>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0manimacao_episodio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRecompensa total: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Passos até o estado terminal: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-efc7a86642ae>\u001b[0m in \u001b[0;36manimacao_episodio\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ação: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recompensa: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# inicializacao\n","env.reset()\n","frames = [] # animacao\n","rec_total = 0\n","epochs = 0\n","\n","s = env.reset()\n","epochs, rec_total_i = 0,0\n","fim = False\n","while not fim:\n","    a = np.argmax(q_table[s])\n","    s, r, fim, info = env.step(a)\n","    epochs += 1\n","    rec_total += r\n","\n","    frames.append({\n","        'frame': env.render(mode='ansi'),\n","        'state': s,\n","        'action': a,\n","        'reward': r\n","        }\n","    )\n","\n","env.close()\n","\n","animacao_episodio(frames)\n","print(\"\\nRecompensa total: \", rec_total)\n","print(\"Passos até o estado terminal: \", epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKo50uxf0cka"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}