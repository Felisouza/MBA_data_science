{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K8CDQUj8yqpq"},"source":["## MBA em Ciência de Dados\n","# Redes Neurais e Arquiteturas Profundas\n","\n","### <span style=\"color:darkred\">Módulo 8 -  Introdução ao Aprendizado por Reforço</span>\n","\n","\n","### <span style=\"color:darkred\">Avaliação</span>\n","\n","Moacir Antonelli Ponti\n","\n","CeMEAI - ICMC/USP São Carlos\n","\n","---\n","\n","As respostas devem ser dadas no Moodle, use esse notebook apenas para gerar o código necessário para obter as respostas\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"xMJ4IFd7yqpt"},"source":["### Questão 1)\n","\n","Qual das alternativas melhor descreve o processo de treinamento de um modelo de aprendizado por reforço?\n","\n","(a) Aprender a classificar exemplos anotados de uma base de dados, de forma a  possibilitar inferir ações futuras para exemplos não vistos durante o treinamento<br>\n","(b) Buscar em uma base de ações disponíveis aquela que seja mais similar à ações anteriores de forma a ter maior estabilidade nas ações futuras<br>\n","(c) Encontrar similaridades em um espaço de ações e estados, de forma que seja possível entender como se agrupam as ações, ou seja, quais subconjuntos de ações e estados são similares, e quais são distintos, permitindo agrupá-los pelo nível de recompensa. <br>\n","(d) Selecionar as ações em cada estado que maximize a recompensa futura com base em múltiplos experimentos simulados<br>\n","(e) Aprender usando busca exaustiva, avaliando em cada estado todas as ações possíveis e, em cada episódio, caso uma ação gere baixa recompensa, voltar atrás nos estados anteriores para rever as ações tomadas e maximizar a recompensa naquele episódio.<br>\n"]},{"cell_type":"markdown","source":["---\n","## Questão 2)\n","\n","Quais algoritmos representam as duas principais abordagens no treinamento de modelos de aprendizado por reforço?\n","\n"," (a) Otimização da Máxima Recompensa e Algoritmo de Ambiente-Modelo-Estado<br>\n"," (b) Actor-Critic e Transformer network<br>\n"," (c) Aprendizado de políticas de seleção de ações e Aprendizado do Valor<br>\n"," (d) Adam e Momentum<br>\n"," (e) Simulated Stochastic Gradient Descent (SSGD) <br>\n"],"metadata":{"id":"2LqA-G9qLDpK"}},{"cell_type":"markdown","metadata":{"id":"nJQ0-S3myqqL"},"source":["---\n","### Questão 3)\n","\n","Ao usar uma rede neural para otimizar algoritmos de aprendizado por reforço, considerando Policy Learning e Value Learning, dado como entrada um estado atual, como deve ser projetada a tarefa de forma a gerar uma saída da rede neural condizente com essas abordagens?\n","\n"," (a) Policy: regressão do valor de recompensa de todas as ações disponíveis. Value: classificação selecionando a ação de maior valor futuro; <br>\n"," (b) Policy: classificação da ação que obterá a menor recompensa, de forma a evitá-la. Value: classificação da ação de maior recompensa selecionando-a diretamente<br>\n"," (c) Policy: classificação do próximo estado. Value: regressão do valor de recompensa de todas as ações disponíveis<br>\n"," (d) Policy: classificação da política que gerará a melhor estratégia. Value: classificação da ação que obterá a maior recompensa<br>\n"," (e) Policy: classificação da ação geradora da maior recompensa. Value: regressão do valor de recompensa de todas as ações disponíveis<br>"]},{"cell_type":"markdown","metadata":{"id":"C1sh5GgYyqqY"},"source":["---\n","\n","### Questão 4)\n","\n","Usando as biblioteca `gymnasium` carregue os ambientes \"CartPole-v1\", \"LunarLander-v3\" e \"BipedalWalker-v3\".\n","\n","Como é o espaço de ações possíveis desses ambientes?\n","\n","(a) CartPole: discreto com 4 ações, LunarLander: discreto com 3 ações, Bipedal Walker: contínuo com valores entre 0.0, 1.0 <br>\n","(b) CartPole: discreto com 3 ações, LunarLander: discreto com 4 ações, Bipedal Walker: discreto com 4 ações<br>\n","(c) CartPole: discreto com 3 ações, LunarLander: discreto com 3 ações, Bipedal Walker: contínuo com valores entre -1.0, 1.0<br>\n","(d) CartPole: discreto com 3 ações, LunarLander: discreto com 4 ações, Bipedal Walker: contínuo com valores entre 0.0, 1.0<br>\n","(e) CartPole: discreto com 2 ações, LunarLander: discreto com 4 ações, Bipedal Walker: contínuo com valores entre -1.0, 1.0<br>\n"]},{"cell_type":"code","source":["%%capture\n","!pip install swig\n","!pip install gymnasium[box2d]"],"metadata":{"id":"-mYJpMpnUGVH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym"],"metadata":{"id":"CzWnYhLJcAl2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRbSyAXeUwaq"},"source":["---\n","\n","### Questão 5)\n","\n","Carregue o ambiente `FrozenLake-v1` conforme o código do notebook. Este ambiente faz parte dos ambientes Toy Text, que contêm informações gerais sobre o ambiente.\n","\n","Frozen Lake envolve atravessar um lago congelado do início ao objetivo sem cair em nenhum buraco, andando sobre o lago congelado. O jogador pode nem sempre se mover na direção pretendida devido à natureza escorregadia do lago congelado.\n","\n","Espaço de Ação: são possíveis as seguintes ações, indicando em que direção mover o jogador.\n","    0: Mover para a esquerda\n","    1: Mover para baixo\n","    2: Mover para a direita\n","    3: Mover para cima\n","\n","Espaço de Observação: a observação é um valor que representa a posição atual do jogador como `current_row * nrows + current_col` (onde tanto a linha quanto a coluna começam em 0). Por exemplo, a posição do objetivo em um mapa de tamanho 8x8 pode ser calculada da seguinte forma: `7 * 8 + 7 = 63`. O número de observações possíveis depende do tamanho do mapa.\n","\n","Recompensas:\n","    - Alcançar o objetivo: +1\n","    - Chegar a um buraco: 0\n","    - Chegar a uma área congelada: 0\n","\n","Estados de Fim do Episódio:\n","- O jogador se move para um buraco.\n","- O jogador alcança o objetivo (posição específica do mapa)\n","\n","Execute 100 mil episódios aleatórios, amostrando aleatoriamente (sample) do espaço de ações. Cada episódio deve executar até chegar ao fim.\n","\n","Calcule e indique a alternativa correta dentro ds intervalos de:\n","- Média de ações por episódio (MA)\n","- Porcentagem de episódios com sucesso (PS)\n","- Porcentagem de episódios com falha (PF)\n","\n","Para as porcentagens, considere valores do tipo XXX%, arredondando casas decimais, por ex.: 5%, 100%.\n","\n","(a) MA = [11,18]; PS = [0%,3%]; PF = [97%,100%]<br>\n","(b) MA = [20,26]; PS = [15%,20%]; PF = [80%,85%]<br>\n","(c) MA = [20,26]; PS = [0%,3%]; PF = [97%,100%]<br>\n","(d) MA = [29,35]; PS = [0%,3%]; PF = [97%,100%]<br>\n","(e) MA = [29,35]; PS = [15%,20%]; PF = [80%,85%]<br>"]},{"cell_type":"code","source":["%%capture\n","!pip install gymnasium"],"metadata":{"id":"cS7uPoLHeLph"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTm0H0TIUwar"},"source":["import gymnasium as gym\n","\n","env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")\n","\n","\n","for i in range():\n","    ####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgtagSTWUwas"},"source":[],"execution_count":null,"outputs":[]}]}