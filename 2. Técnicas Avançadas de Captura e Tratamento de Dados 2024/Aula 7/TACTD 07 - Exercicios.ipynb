{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_pmxiT-nVwC"
   },
   "source": [
    "# MBA em Ciência de Dados\n",
    "## Técnicas Avançadas de Captura e Tratamento de Dados\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VII - Dados não estruturados: sinais e imagens</span>\n",
    "\n",
    "### <span style=\"color:darkred\">Exercícios</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "#### <span style=\"color:red\">Recomenda-se fortemente que os exercícios sejam feitos sem consultar as respostas antecipadamente.</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DAPToahynVwF"
   },
   "outputs": [],
   "source": [
    "# carregando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtirsEQ_nVwH"
   },
   "source": [
    "### Exercício 1)\n",
    "\n",
    "Quando comparamos imagens e sinais e suas características, o que devemos considerar a priori?\n",
    "\n",
    "(a) Sinais possuem valores independente e identicamente distribuídos, enquanto Imagens possuem pixels organizados de forma espacial<br>\n",
    "(b) Sinais possuem valores codificados em 16 bits, enquanto imagens possuem valores codificados em 8 bits<br>\n",
    "(c) Sinais possuem valores com dependência sequencial, enquanto imagens não possuem padrão de dependência<br>\n",
    "(d) Sinais possuem valores com dependência sequencial, enquanto Imagens possuem dependência espacial de seus valores<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-PUIrVTnVwI"
   },
   "source": [
    "### Exercício 2)\n",
    "\n",
    "Sejam sinais representados por 32 bits (int32). Quantos valores distintos é possível representar?\n",
    "\n",
    "(a) 32,76 Mil<br>\n",
    "(b) 65,53 Mil<br>\n",
    "(c) 2,14 Bilhões<br>\n",
    "(d) 4,29 Bilhões<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fe0qQfcC8dx"
   },
   "source": [
    "### Exercício 3)\n",
    "\n",
    "Sejam pixeis de imagens representados por 16 bits sem sinal (uint16). Quantos valores distintos é possível representar em cada pixel?\n",
    "\n",
    "\n",
    "(a) 32,76 Mil<br>\n",
    "(b) 65,53 Mil<br>\n",
    "(c) 2,14 Bilhões<br>\n",
    "(d) 4,29 Bilhões<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfmxUICXnVwJ"
   },
   "source": [
    "### Exercício 4)\n",
    "\n",
    "Carregue os dados do arquivo `sinais2.csv` utilizando\n",
    "\n",
    "`signals = np.genfromtxt(arquivo, delimiter=',').astype(np.float32)`.\n",
    "\n",
    "O array resultante possui um sinal por linha, i.e. `sinal[i]`\n",
    "\n",
    "Utilizando os sinais carregados utilize a `np.fft.fft()` para obter a Transformada de Fourier dos sinais. Depois, considerando apenas frequências até 50 Hz, calcule quais são as 4 frequências de maior valor de magnitude (maginute calculada pelo `np.abs()`). Aqui não queremos os valores da magnitude, mas a quais frequências (índices) elas se referem. Para complementar a análise, plote as magnitudes das transformadas até a frequência 50.\n",
    "\n",
    "Analisando as frequências de maior magnitude temos as frequências que mais caracterizam o sinal. Considerando as 4 frequências computadas anteriormente, podemos dividir os sinais em categorias distintas. Nesse sentido, qual análise abaixo está correta?\n",
    "\n",
    "(a) O sinal 4 possui frequências inferiores quando comparado com os demais, indicando que o sinal 4 é provavalmente  dependente sequencialmente, enquanto os demais são i.i.d.; assim podemos dividí-los em duas categorias: sinal 4 e sinais 0, 1, 2 e 3.<br>\n",
    "(b) O sinal 3 possui frequências mais significativas 20 Hz ou superior, indicando que é um sinal com maior qualidade de aquisição, e assim podemos categorizar em: sinal 3, e sinais 0, 1, 2 e 4.<br>\n",
    "(c) Todas as frequências estão abaixo de 50 Hz, sendo assim podemos dizer que os sinais são todos similares, sendo impossível dividí-los em categorias.<br>\n",
    "(d) O sinal 3 possui frequências mais significativas 20 Hz ou superior, possuindo transições mais rápidas de valores do que os outros com frequências caracerísticas menores do que 12Hz; e assim podemos categorizar em: sinal 3, e sinais 0, 1, 2 e 4.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n8-friLTnVwJ",
    "outputId": "6dccc5c7-1677-433b-874c-bb706dd6ffb4"
   },
   "outputs": [],
   "source": [
    "signals = np.genfromtxt('./dados/sinais2.csv', delimiter=',').astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3U89pqOnVwL"
   },
   "source": [
    "### Exercício 5)\n",
    "\n",
    "Considerando os mesmos sinais carregados no exercício anterior, compute as características: entropia da energia (com 10 blocos), taxa de cruzamentos por zero, entropia espectral (com 10 blocos), formando um vetor com 3 características para cada sinal.\n",
    "\n",
    "Após isso, compute a matriz de distâncias entre os sinais considerando a distância L1, i.e., a soma dos valores absolutos das diferenças entre dois vetores $A$ e $B$:\n",
    "\n",
    "$$\\sum_i |A_i - B_i|$$\n",
    "\n",
    "Da matriz, que indica a dissimilaridade entre pares de sinais, aplique uma soma na direção do eixo 0 (axis=0) e depois arredonde para inteiro `np.round(,0)`. Quais valores foram obtidos para cada sinal?\n",
    "\n",
    "(a) Sinais 0, 1, 2 e 4, soma 2; Sinal 3, soma 6.<br>\n",
    "(b) Sinais 0 e 4, soma 3; Sinais 1 e 2, soma 2; Sinal 3, soma 6.<br>\n",
    "(c) Sinais 0, 1, e 2, soma 2; Sinal 3, soma 6; Sinal 4, soma 3.<br>\n",
    "(d) Sinais 0, 1, e 2, soma 1; Sinal 3, soma 3; Sinal 4, soma 6.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyy5hV7lnVwW"
   },
   "source": [
    "### Exercício 6)\n",
    "\n",
    "Qual a principal motivação do uso de modelos pré-treinados de deep learning (redes neurais profundas) como extratores de características para imagens?\n",
    "\n",
    "(a) deep learning é a metodologia de aprendizado mais popular atualmente, sendo usada em várias aplicações com alta taxa de sucesso<br>\n",
    "(b) métodos de extração de características manuais/handcrafted são irrelevantes atualmente e não funcionam bem na maioria dos casos<br>\n",
    "(c) esses modelos são pré-treinados em bases de dados grandes, aprendendo características úteis para distintas aplicações<br>\n",
    "(d) existem muitos modelos disponíveis para download, permitindo realizar diferentes experimentos e encontrar o modelo mais adequado para o problema alvo<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ah9Aptg13UTz"
   },
   "source": [
    "### Exercício 7)\n",
    "\n",
    "1. Carregar imagens conforme abaixo e salve em um array, considere seus índices como 0, 1 e 2, nessa ordem (código já pronto abaixo).\n",
    "2. Carregar o modelo InceptionV3 (Ver em https://keras.io/api/applications/inceptionv3/) como modelo pré-treinado base\n",
    "3. Montar uma rede neural extratora contendo as camadas:\n",
    "    - camada de entrada\n",
    "    - camada de pré-processamento (método `preprocess_input` relativo ao InceptionV3)\n",
    "    - modelo base pré-treinado\n",
    "    - camada de GlobalAveragePooling2D\n",
    "4. Extrair os embeddings com base no modelo montado no passo anterior, e achatar os embeddings gerando vetores de características\n",
    "5. Calcule a matriz de distância Euclidiana entre todas as imagens utilizando os vetores obtidos\n",
    "6. Verifique quais imagens são mais similares entre si, considerando seus índices no dataset, e qual a distância entre eles\n",
    "\n",
    "(a) 0 e 1, distância 30<br>\n",
    "(c) 1 e 2, distância 30<br>\n",
    "(d) 0 e 1, distância 33<br>\n",
    "(e) 1 e 2, distância 33 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = imageio.imread(\"dados/photo_flower1.jpg\")\n",
    "img2 = imageio.imread(\"dados/photo_flower2.jpg\")\n",
    "img3 = imageio.imread(\"dados/photo_ukulele1.jpg\")\n",
    "\n",
    "ds_images = np.stack([img1, img2, img3])\n",
    "ds_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcZ5446wnVwP"
   },
   "source": [
    "### Exercício 8)\n",
    "\n",
    "1. Carregar imagens conforme abaixo e salve em um array, considere seus índices como 0, 1 e 2, nessa ordem (código já pronto abaixo).\n",
    "2. Carregar o modelo InceptionV3 (Ver em https://keras.io/api/applications/inceptionv3/) como modelo pré-treinado base\n",
    "3. Montar uma rede neural extratora contendo as camadas:\n",
    "    - camada de entrada\n",
    "    - camada de pré-processamento (método `preprocess_input` relativo ao InceptionV3)\n",
    "    - modelo base pré-treinado\n",
    "4. Extrair os embeddings com base no modelo montado no passo anterior, e achatar os embeddings gerando vetores de características\n",
    "5. Calcule a matriz de distância Euclidiana entre todas as imagens utilizando os vetores obtidos\n",
    "6. Verifique quais imagens são mais similares entre si, considerando seus índices no dataset, e qual a distância entre eles\n",
    "\n",
    "(a) 0 e 1, distância 30<br>\n",
    "(c) 1 e 2, distância 30<br>\n",
    "(d) 0 e 1, distância 220<br>\n",
    "(e) 1 e 2, distância 220 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "p5qlv0nVJiKK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = imageio.imread(\"dados/photo_flower1.jpg\")\n",
    "img2 = imageio.imread(\"dados/photo_flower2.jpg\")\n",
    "img3 = imageio.imread(\"dados/photo_ukulele1.jpg\")\n",
    "\n",
    "ds_images = np.stack([img1, img2, img3])\n",
    "ds_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
