{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwT2hdjUKbdD"
   },
   "source": [
    "# <font color=\"blue\"> MBA em Ciência de Dados</font>\n",
    "# <font color=\"blue\">Técnicas Avançadas para Captura e Tratamento de Dados</font>\n",
    "\n",
    "## <font color=\"blue\"> Matriz Documento $\\times$ Palavras - Bag of Words</font>\n",
    "    \n",
    "## <font color=\"blue\">Exercícios</font>\n",
    "\n",
    "**Material Produzido por Luis Gustavo Nonato**<br>\n",
    "**Cemeai - ICMC/USP São Carlos**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7ssuoGTKbdG"
   },
   "source": [
    "<font color='red'> Recomendamos fortemente resolver os exercícios sem consultar as soluções antecipadamente </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvBLI5fcKbdG"
   },
   "source": [
    "Os exercícios abaixo fazem uso da coleção de documentos presente no diretório `DocCol2` contido no arquivo <font style=\"font-family: monaco\"> DocCol.zip</font>, o qual pode ser baixado do Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOCQy595KbdH"
   },
   "source": [
    "### Exercício 1)\n",
    "Armazene os documentos disponíveis no diretório `DocCol2` em um dicionário onde a chave é o nome do arquivo e o valor é a string contida no arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "qyM7uzZdKbdH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_names = glob.glob(\"DocCol2/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {}\n",
    "\n",
    "for file_name in file_names:\n",
    "\n",
    "    # Ler os conteúdos\n",
    "    with open(file_name) as file:\n",
    "        contents = file.read()\n",
    "    \n",
    "    # Extrair nome\n",
    "    base_name = os.path.basename(file_name)\n",
    "    \n",
    "    # Salvar no dicionário\n",
    "    docs[base_name] = contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ7Q-rM1KbdH"
   },
   "source": [
    "### Exercício 2)\n",
    "Percorra as strings armazenadas como valores no dicionário criado no exercício 1), concatenando-as em uma única string. Quebre a string gerada em uma lista de palavras.\n",
    "\n",
    "**DICA**: Utilize <font color='blue'>word_tokenize</font> do pacote <font color='blue'>nltk</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "nzqxSUJFKbdI"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    " \n",
    "words = \" \".join(docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce4GDOHCKbdI"
   },
   "source": [
    "### Exercício 3)\n",
    "Remova todas os \"tokens\" da lista criada no exercício 2) que sejam de comprimento 1 ou que contenham caractéres que não são letras do alfabeto. Faça com que todas as palavras estajam em letras minúsculas na lista resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "FRnVKph1KbdI"
   },
   "outputs": [],
   "source": [
    "words_filtered = [\n",
    "    word.lower() for word in words\n",
    "    if len(word) > 1 and word.isalpha() \n",
    "]\n",
    "\n",
    "# words_filtered = []\n",
    "# for word in words:\n",
    "#     if len(word) > 1 and word.isalpha():\n",
    "#         words_filtered.append(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAcMp6EJKbdI"
   },
   "source": [
    "### Exercício 4)\n",
    "Remova da lista de palavras resultante do exercício 3) todas as \"stop_words\". Lembre-se que as palavras são da língua inglesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "AjAsAY9WKbdI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 752/752 [00:00<00:00, 1323035.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "words_filtered = [\n",
    "    word for word in tqdm(words_filtered)\n",
    "    if word not in stop_words\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDjWyOj1KbdJ"
   },
   "source": [
    "### Exercício 5) \n",
    "Faça a normalização léxica (stemming) das palavras da lista resultante do exercício 4) e remova palavras repetidas após a normalização. Quantas palavras foram removidas após a normalização léxica?\n",
    "\n",
    "**Dica**: Utilize o método <font color='blue'>PorterStemmer</font> do pacote <font color='blue'>nltk.stem</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "WEY4YSwcKbdJ"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words_stemmed = [\n",
    "    stemmer.stem(word) for word in words_filtered\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "radicals_set = set(words_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6781"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(radicals_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9634"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(words_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMUJR5VFKbdJ"
   },
   "source": [
    "### Exercício 6) \n",
    "Gere uma lista de palavras para cada documento armazenado como valores no dicionário do exercício 1) aplicando os passos dos exercícios de 2 a 5 para cada documento.\n",
    "Porém, no passo 5 não remova palavras repetidas após a normalização léxica. Armazene o resultado em um novo dicionário onde as chaves são os nomes dos documentos, como no dicionário do exercício 1), e os valores são as listas de palavras geradas para cada documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkHpkt7XKbdJ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "OGLmdofxKbdK"
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for filename, contents in docs.items():\n",
    "    \n",
    "    # Separar por palavras\n",
    "    words = nltk.word_tokenize(contents)\n",
    "    \n",
    "    # Remover caracteres especiais e deixar em minúsculo\n",
    "    words_filtered = [\n",
    "        word.lower() for word in words\n",
    "        if len(word) > 1 and word.isalpha() \n",
    "    ]\n",
    "    \n",
    "    # Remover as stop words\n",
    "    words_filtered = [\n",
    "        word for word in words_filtered\n",
    "        if word not in stop_words\n",
    "    ]\n",
    "    \n",
    "    # Extrair os radicais\n",
    "    words_stemmed = [\n",
    "        stemmer.stem(word) for word in words_filtered\n",
    "    ]\n",
    "    \n",
    "    data[filename] = words_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBa8V9GVKbdK"
   },
   "source": [
    "### Exercício 7)\n",
    "Utilize o dicionário contruído no exercício anterior para construir\n",
    "uma matriz Documentos $\\times$ Palavras para a coleção de documentos contidos no diretório `DocCol2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "XTPqCjXWKbdK"
   },
   "outputs": [],
   "source": [
    "radicals = list(radicals_set)\n",
    "documents = list(data.keys())\n",
    "\n",
    "n_documents = len(documents)\n",
    "n_words = len(radicals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 6781)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz = np.zeros((n_documents, n_words))\n",
    "matriz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [00:08<00:00, 11.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(n_documents)):\n",
    "    for word in data[documents[i]]:\n",
    "        j = radicals.index(word)\n",
    "        \n",
    "        matriz[i][j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRztnWzSKbdK"
   },
   "source": [
    "### Exercício 8)\n",
    "Utilizando PCA, projete a coleção de documentos no espaço gerado pelas duas direções principais da matriz Documentos $\\times$ Palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFrL0bECKbdK"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uddc6z38KbdK"
   },
   "source": [
    "### Exercício 9)\n",
    "Visualize a projeção da coleção de documentos no espaço bidimensional gerado no exercício anterior e se necessário faça um zoom para melhor visualizar a distribuição dos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B16wd6ZRKbdL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILBcg3U3KbdL"
   },
   "source": [
    "### Exercício 10)\n",
    "Dado o documento `au8` (linha de rótulo `au8` no DataFrame Documentos $\\times$ Palavras) e utilizando a distância \"cosseno\"  (cosseno entre dois vetores correspondendo a vetorização dos documentos), encontre qual o documento mais parecido com `au8`. Calcule o cosseno entre a linha representada por `au8` na matriz Documentos $\\times$ Palavras e todas as outras linhas, tomando o maior valor como correspondendo ao documento mais parecido.\n",
    "Repita a projeção realizada no exercício 8), ressaltando o documento `au8` e o documento mais parecido com ele. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIm5wNRJKbdL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "docXpala exercicios.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
