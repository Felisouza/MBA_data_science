{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e106c80d-4997-4157-98dc-ebcd216849c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "flekT6GFDN6m"
   },
   "source": [
    "# <span style=\"color:blue\">MBA em Ciência de Dados</span>\n",
    "# <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n",
    "\n",
    "## <span style=\"color:blue\">Avaliação Final - Notebook </span>\n",
    "\n",
    "**Material Produzido por:**<br>\n",
    ">**Profa. Dra. Cristina Dutra de Aguiar**<br>\n",
    "\n",
    "\n",
    "**CEMEAI - ICMC/USP São Carlos**\n",
    "\n",
    "Este *notebook* deve conter as respostas para as consultas analíticas solicitadas nas Questões 7, 8, 9 e 10. É possível especificar as consultas analíticas usando Pandas, o método spark.sql() ou os métodos do módulo pyspark.sql. As consultas devem ser especificadas na seção 6, na célula indicada para resposta.\n",
    "\n",
    "**IMPORTANTE**\n",
    "\n",
    "- **As respostas para as Questões 7, 8, 9 e 10 somente serão consideradas se forem especificadas no *notebook*. Portanto, independentemente da alternativa estar certa ou errada, se a consulta analítica correspondente não for especificada, a alternativa será considerada errada.**\n",
    "\n",
    "- **Caso uma alternativa esteja certa, porém a especificação da consulta analítica correspondente estiver errada no *notebook*, a alternativa será considerada errada.**\n",
    "\n",
    "\n",
    "O *notebook* contém a constelação de fatos da BI Solutions que deve ser utilizada para responder às questões e também todas as bibliotecas, bases de dados, inicializações, instalações, importações, geração de dataFrames, geração de visões temporárias e conversão dos tipos de dados necessárias para a realização da questão. Portanto, o *notebook* está preparado para ser executado usando Pandas, o método spark.sql() e os métodos do módulo pyspark.sql.\n",
    "\n",
    "O uso do *framework* Spark requer diversas configurações no ambiente de desenvolvimento para executar o *notebook*. Dado que tal complexidade foge do escopo de nossa disciplina, recomenda-se que o *notebook* seja executado na plataforma de desenvolvimento COLAB. O uso do COLAB  proporciona um ambiente de desenvolvimento pré-configurado e remove a complexidade de instalação e configuração de pacotes e *frameworks* que são utilizados na disciplina.\n",
    "\n",
    "**INSTRUÇÕES DE ENTREGA**\n",
    "\n",
    "**O que deve ser entregue:**\n",
    "- **O notebook com as respostas no formato .ipynb**\n",
    "- **O notebook com as respostas no formato .pdf**\n",
    "\n",
    "**Ambos arquivos devem ser nomeados usando o primeiro nome e o último sobrenome do aluno. Por exemplo: CristinaAguiar.ipynb e CristinaAguiar.pdf.**\n",
    "\n",
    "Boa avaliação!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1679e96c-03ec-4971-abd8-dc79f72d9053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3o3dN_WLQcyD"
   },
   "source": [
    "#1 Constelação de Fatos da BI Solutions\n",
    "\n",
    "A aplicação de *data warehousing* da BI Solutions utiliza como base uma constelação de fatos, conforme descrita a seguir.\n",
    "\n",
    "**Tabelas de dimensão**\n",
    "\n",
    "- data (dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno)\n",
    "- funcionario (funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla)\n",
    "- equipe (equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla)\n",
    "- cargo (cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel)\n",
    "- cliente (clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla)\n",
    "\n",
    "**Tabelas de fatos**\n",
    "- pagamento (dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamentos)\n",
    "- negociacao (dataPK, equipePK, clientePK, receita, quantidadeNegociacoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01e4cf97-e9d0-49ef-84ac-2489bebb9428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "BGeh8KdXwVCQ"
   },
   "source": [
    "#2 Obtenção dos Dados da BI Solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0359fa0-c0f2-49cf-8a8a-bc426987f932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CCCNC64AzBG0"
   },
   "source": [
    "## 2.1 Baixando o Módulo wget\n",
    "\n",
    "Para baixar os dados referentes ao esquema relacional da constelação de fatos da BI Solutions, é utilizado o módulo  **wget**. O comando a seguir realiza a instalação desse módulo. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85c07094-1084-4073-9566-42f7f19789c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3e0Eao1K0EYG"
   },
   "outputs": [],
   "source": [
    "#instalando o módulo wget\n",
    "%%capture\n",
    "!pip install -q wget\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2525995-9b39-4327-9ec3-a32990f04aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "j56pVJ2hZ2i5"
   },
   "source": [
    "## 2.2 Obtenção dos Dados das Tabelas de Dimensão\n",
    "\n",
    "Os comandos a seguir baixam os dados que povoam as tabelas de dimensão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e92f29-bdb6-4a52-a426-01b06b63a894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1415,
     "status": "ok",
     "timestamp": 1731622199578,
     "user": {
      "displayName": "Cristina Aguiar",
      "userId": "02805498492766545440"
     },
     "user_tz": 180
    },
    "id": "46QzTpLJwfkW",
    "outputId": "b92750d6-8d91-48be-8ca5-ce631294d4d0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'data/cliente.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baixando os dados das tabelas de dimensão\n",
    "import wget\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv\"\n",
    "wget.download(url, \"data/data.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv\"\n",
    "wget.download(url, \"data/funcionario.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv\"\n",
    "wget.download(url, \"data/equipe.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv\"\n",
    "wget.download(url, \"data/cargo.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv\"\n",
    "wget.download(url, \"data/cliente.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "898e950c-c7af-4b24-872b-d4ecdf571ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0o-dC7feszRc"
   },
   "source": [
    "## 2.3 Obtenção dos Dados Tabelas de Fatos\n",
    "\n",
    "Os comandos a seguir baixam os dados que povoam as tabelas de fatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e53f76e-382a-44f4-83a6-2607884c17b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1731622200519,
     "user": {
      "displayName": "Cristina Aguiar",
      "userId": "02805498492766545440"
     },
     "user_tz": 180
    },
    "id": "XWM-CUFgBl_8",
    "outputId": "b86e1103-e723-4bda-8969-ab3fa0580b7e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'data/negociacao.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baixando os dados das tabelas de fatos\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv\"\n",
    "wget.download(url, \"data/pagamento.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv\"\n",
    "wget.download(url, \"data/negociacao.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c5e641b-b8ea-4b94-8867-fe73df0898d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sO16-7-jOioq"
   },
   "source": [
    "# 3 Apache Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6acb9d4-1878-401d-a0d9-1049c6b61195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "YVEgY9qKflBV"
   },
   "source": [
    "## 3.1 Instalação\n",
    "\n",
    "Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b71417a9-1975-43ad-af17-b090d490161d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KaM-OnIjgLS2"
   },
   "source": [
    "Para que o cluster possa ser criado, primeiramente é instalado o Java Runtime Environment (JRE) versão 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d076e52-2a0b-4d1d-a9c0-49f4e2d1812d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NXls3bfoglKW"
   },
   "outputs": [],
   "source": [
    "#instalando Java Runtime Environment (JRE) versão 8\n",
    "%%capture\n",
    "!apt-get remove openjdk*\n",
    "!apt-get update --fix-missing\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ff44f27-e1bb-4dcb-a53b-eb2f9c0acaeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7BQzZfDYhb4j"
   },
   "source": [
    "Na sequência, é feito o *download* do Apache Spark versão 3.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99621874-76e5-474f-af78-fe0550ff2b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8a_Yv59zg3gm"
   },
   "outputs": [],
   "source": [
    "#baixando Apache Spark versão 3.0.0\n",
    "%%capture\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
    "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69ac1b8d-9f7d-431a-b699-189774874cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "RETWX6wqhkLf"
   },
   "source": [
    "Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "735a1c9b-4cb3-48aa-8ad7-b28b37e84bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "iZpR7NwOh2EB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#configurando a variável de ambiente JAVA_HOME\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "#configurando a variável de ambiente SPARK_HOME\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41384275-11a6-4249-bb27-dd0cb70434c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Ql0z7Ro1iHQb"
   },
   "source": [
    "Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n",
    "\n",
    "> **Pacote findspark:** Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark.\n",
    "\n",
    "> **Pacote pyspark:** PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o *framework* Apache Spark encontra-se desenvolvido na linguagem de programação Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0779e45-5940-474d-97e2-a39f15133f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5oSYOwKljPf5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#instalando o pacote findspark\n",
    "!pip install -q findspark==1.4.2\n",
    "#instalando o pacote pyspark\n",
    "!pip install -q pyspark==3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20448dd6-b883-4c2f-ac38-91a6f34dfdf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "eAaLyjPzmIwZ"
   },
   "source": [
    "## 3.2 Conexão\n",
    "\n",
    "PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo.\n",
    "\n",
    "Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e1b981-2bad-4bd4-a404-487104b94634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-zm1pBTEmjp4"
   },
   "outputs": [],
   "source": [
    "#importando o módulo findspark\n",
    "import findspark\n",
    "#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1eb4535-a2a6-4564-8890-98cab232fcf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZDqfefF7YUab"
   },
   "source": [
    "Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível iniciar o uso do Spark na aplicação de `data warehousing`. Para tanto, é necessário importar o comando `SparkSession` do módulo `pyspark.sql`. São utilizados os seguintes conceitos: <br>\n",
    "\n",
    "- `SparkSession`: permite a criação de `DataFrames`. Como resultado, as tabelas relacionais podem ser manipuladas por meio de `DataFrames` e é possível realizar consultas OLAP por meio de comandos SQL. <br>\n",
    "- `builder`: cria uma instância de SparkSession. <br>\n",
    "- `appName`: define um nome para a aplicação, o qual pode ser visto na interface de usuário web do Spark. <br>\n",
    "- `master`: define onde está o nó mestre do *cluster*. Como a aplicação é executada localmente e não em um *cluster*, indica-se isso pela *string* `local` seguida do parâmetro `[*]`. Ou seja, define-se que apenas núcleos locais são utilizados.\n",
    "- `getOrCreate`: cria uma SparkSession. Caso ela já exista, retorna a instância existente.\n",
    "\n",
    "\n",
    "**Observação**: A lista completa de todos os parâmetros que podem ser utilizados na inicialização do *cluster* pode ser encontrada neste [link](https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c0f7866-ce8e-4cc5-b1ef-9424a223d718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9TxljJ_cwBCy"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ddfce76-15d1-4b01-82b1-1ceabae000c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2IpYfoG_kx_8"
   },
   "source": [
    "# 4 Geração dos DataFrames em Pandas da BI Solutions\n",
    "\n",
    "Nesta seção são gerados os DataFrames em Pandas. Atenção aos nomes desses DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "389ffa54-1660-4af9-b6ac-8d1bbdac23cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9arYf_PHlJCR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e32b5a72-bf25-4ea9-9e72-94377e4737cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qw4NfyDZ--6z"
   },
   "outputs": [],
   "source": [
    "cargoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv')\n",
    "clientePandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv')\n",
    "dataPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv')\n",
    "equipePandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv')\n",
    "funcionarioPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv')\n",
    "negociacaoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv')\n",
    "pagamentoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdf27c06-fef3-4416-b88f-9109682784d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5qL9SiR_pQE2"
   },
   "source": [
    "# 5 Geração dos DataFrames em Spark da BI Solutions\n",
    "\n",
    "Nesta seção são gerados dos DataFrames em Spark. Atenção aos nomes desses DataFrames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "254eeef1-8edd-4c75-ae87-f83cf3b0c9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DRVoz-SGt87W"
   },
   "source": [
    "## 5.1 Criação dos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9897a8cc-e5c6-4749-a041-fa481ba425b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "cellView": "both",
    "id": "FNR-3dV6oYk4"
   },
   "outputs": [],
   "source": [
    "#criando os DataFrames em Spark\n",
    "cargo = spark.read.csv(path=\"data/cargo.csv\", header=True, sep=\",\")\n",
    "cliente = spark.read.csv(path=\"data/cliente.csv\", header=True, sep=\",\")\n",
    "data = spark.read.csv(path=\"data/data.csv\", header=True, sep=\",\")\n",
    "equipe = spark.read.csv(path=\"data/equipe.csv\", header=True, sep=\",\")\n",
    "funcionario = spark.read.csv(path=\"data/funcionario.csv\", header=True, sep=\",\")\n",
    "negociacao = spark.read.csv(path=\"data/negociacao.csv\", header=True, sep=\",\")\n",
    "pagamento = spark.read.csv(path=\"data/pagamento.csv\", header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d88f03a-b323-482c-8cca-68778d280d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mrch9vLgjl_H"
   },
   "source": [
    "## 5.2 Atualização dos Tipos de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc1b5ebf-ff2a-4308-91c0-66d172b9c70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9A_ot2pOjsWB"
   },
   "source": [
    "Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado inteiro. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cca20ca9-4219-4c32-a918-f395444e67b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jmCV6Mur__z6"
   },
   "outputs": [],
   "source": [
    "# identificando quais colunas de quais DataFrames devem ser do tipo de dado inteiro\n",
    "colunas_cargo = [\"cargoPK\"]\n",
    "colunas_cliente = [\"clientePK\"]\n",
    "colunas_data = [\"dataPK\", \"dataDia\", \"dataMes\", \"dataBimestre\", \"dataTrimestre\", \"dataSemestre\", \"dataAno\"]\n",
    "colunas_equipe = [\"equipePK\"]\n",
    "colunas_funcionario = [\"funcPK\", \"funcDiaNascimento\", \"funcMesNascimento\", \"funcAnoNascimento\"]\n",
    "colunas_negociacao = [\"equipePK\", \"clientePK\", \"dataPK\", \"quantidadeNegociacoes\"]\n",
    "colunas_pagamento = [\"funcPK\", \"equipePK\", \"dataPK\", \"cargoPK\", \"quantidadeLancamentos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4626c246-f873-4d1e-8d0c-f4dc57e5aeab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yPNnDJcG9R5H"
   },
   "outputs": [],
   "source": [
    "# importando o tipo de dado desejado\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "# atualizando o tipo de dado das colunas especificadas\n",
    "# substituindo as colunas já existentes\n",
    "\n",
    "for coluna in colunas_cargo:\n",
    "  cargo = cargo.withColumn(coluna, cargo[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_cliente:\n",
    "  cliente = cliente.withColumn(coluna, cliente[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_data:\n",
    "  data = data.withColumn(coluna, data[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_equipe:\n",
    "  equipe = equipe.withColumn(coluna, equipe[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_funcionario:\n",
    "  funcionario = funcionario.withColumn(coluna, funcionario[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_negociacao:\n",
    "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_pagamento:\n",
    "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "378c7fb0-3654-49d5-8747-56fe93210fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "J0dX_7U_AzIY"
   },
   "source": [
    "Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado número de ponto flutuante. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91a3e8e3-9847-43d5-8291-f51c735684fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "RBcQ7Ep7AWqN"
   },
   "outputs": [],
   "source": [
    "# identificando quais colunas de quais DataFrames devem ser do tipo de dado número de ponto flutuante\n",
    "colunas_negociacao = [\"receita\"]\n",
    "colunas_pagamento = [\"salario\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c7fc11f-d8f8-4d48-a4f2-f074a5f025bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rcfvkIK1BRSp"
   },
   "outputs": [],
   "source": [
    "# importando o tipo de dado desejado\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "# atualizando o tipo de dado das colunas especificadas\n",
    "# substituindo as colunas já existentes\n",
    "\n",
    "for coluna in colunas_negociacao:\n",
    "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(FloatType()))\n",
    "\n",
    "for coluna in colunas_pagamento:\n",
    "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2df1e1d-fb17-4278-918a-4a5a6f640202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "91nwfsV3_rKs"
   },
   "outputs": [],
   "source": [
    "# importando funções adicionais\n",
    "from pyspark.sql.functions import round, desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bfb7bd5-5af3-46f3-8f7c-bc6d081dc945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9wN5iOGKwnHG"
   },
   "source": [
    "## 5.3 Criação de Visões Temporárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d41c9f6-e52e-4d53-acff-932a17b685c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xJsqRI3TwsjS"
   },
   "outputs": [],
   "source": [
    "#criando as visões temporárias\n",
    "cargo.createOrReplaceTempView(\"cargo\")\n",
    "cliente.createOrReplaceTempView(\"cliente\")\n",
    "data.createOrReplaceTempView(\"data\")\n",
    "equipe.createOrReplaceTempView(\"equipe\")\n",
    "funcionario.createOrReplaceTempView(\"funcionario\")\n",
    "negociacao.createOrReplaceTempView(\"negociacao\")\n",
    "pagamento.createOrReplaceTempView(\"pagamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "941facc6-1500-4458-9457-8d16ab9bc7a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Re - baixando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tive problemas para baixar os dados da forma como a professora baixou e tive que baixar desta maneira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ac8b60c-3f0f-468c-8718-8fe3a1d259ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7c3c52-0744-41c1-b8c5-77bbfede5f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criar a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"ExemploSQL\").getOrCreate()\n",
    "\n",
    "# Lista de URLs\n",
    "lista_url = [\n",
    "    \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv\",\n",
    "    \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv\",\n",
    "    \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv\",\n",
    "    \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv\",\n",
    "    \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv\",\n",
    "    \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv\",\n",
    "    \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv\"\n",
    "]\n",
    "\n",
    "data_pd = pd.read_csv(lista_url[0])\n",
    "data = spark.createDataFrame(data_pd)\n",
    "\n",
    "# Lista de DataFrames dataframes = [df1, df2] # Imprimir o schema de cada DataFrame for i, df in enumerate(dataframes, 1): print(f\"Schema do DataFrame {i}:\") df.printSchema() print(\"\\n\") = spark.createDataFrame(data_pd)\n",
    "\n",
    "func_pd = pd.read_csv(lista_url[1])\n",
    "funcionario = spark.createDataFrame(func_pd)\n",
    "\n",
    "equipe_pd = pd.read_csv(lista_url[2])\n",
    "equipe = spark.createDataFrame(equipe_pd)\n",
    "\n",
    "cargo_pd = pd.read_csv(lista_url[3])\n",
    "cargo = spark.createDataFrame(cargo_pd)\n",
    "\n",
    "cliente_pd = pd.read_csv(lista_url[4])\n",
    "cliente = spark.createDataFrame(cliente_pd)\n",
    "\n",
    "pagamento_pd = pd.read_csv(lista_url[5])\n",
    "pagamento = spark.createDataFrame(pagamento_pd)\n",
    "\n",
    "negociacao_pd = pd.read_csv(lista_url[6])\n",
    "negociacao = spark.createDataFrame(negociacao_pd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe42625-0823-4980-a42f-689c4aef5934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#criando as visões temporárias\n",
    "cargo.createOrReplaceTempView(\"cargo\")\n",
    "cliente.createOrReplaceTempView(\"cliente\")\n",
    "data.createOrReplaceTempView(\"data\")\n",
    "equipe.createOrReplaceTempView(\"equipe\")\n",
    "funcionario.createOrReplaceTempView(\"funcionario\")\n",
    "negociacao.createOrReplaceTempView(\"negociacao\")\n",
    "pagamento.createOrReplaceTempView(\"pagamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d44d409a-5f61-4ae9-9b6d-a2cc460dea0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2X_PNhxFCKh6"
   },
   "source": [
    "# 6 Respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a0a7b1a-c320-493a-953e-ab78e7784495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Questão 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65b25a7-2bcf-4bd0-a17d-1d709c22ffde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpf</th>\n",
       "      <th>nome</th>\n",
       "      <th>preco_viagem_1</th>\n",
       "      <th>preco_viagem_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>204.927.060-77</td>\n",
       "      <td>Joao Paulo</td>\n",
       "      <td>900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cpf</th>\n      <th>nome</th>\n      <th>preco_viagem_1</th>\n      <th>preco_viagem_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>204.927.060-77</td>\n      <td>Joao Paulo</td>\n      <td>900.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pessoas = {\n",
    "    \"cpf\": [\"204.927.060-77\", \"116.948.760-20\", \"327.639.610-61\",\n",
    "            \"904.716.030-40\", \"750.286.140-83\"],\n",
    "    \"nome\": [\"Joao Paulo\", \"Jose Carlos\", \"Maria Eduarda\",\n",
    "             \"Ana Julia\", \"Carlos Alberto\"],\n",
    "    \"idade\": [21, 23, 20, 21, 22],\n",
    "    \"viagem\": [True, True, True, True, False]\n",
    "}\n",
    "\n",
    "df_pessoas = pd.DataFrame(pessoas)\n",
    "\n",
    "viagem1 = {\n",
    "    \"cpf\": [\"204.927.060-77\", \"116.948.760-20\", \"327.639.610-61\", \"904.716.030-40\"],\n",
    "    \"preco_viagem_1\": [900.0, 250.0, 150.0, 8000.0]\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(viagem1)\n",
    "\n",
    "viagem2 = {\n",
    "    \"cpf\": [\"750.286.140-83\", \"116.948.760-20\", \"327.639.610-61\", \"904.716.030-40\"],\n",
    "    \"preco_viagem_2\": [345.0, 4978.0, 8000.0, 9000.0]\n",
    "}\n",
    "\n",
    "df2 = pd.DataFrame(viagem2)\n",
    "\n",
    "dfR = df_pessoas\\\n",
    "    .query(\"viagem == True\")\\\n",
    "    .merge(df1, how=\"left\", on=\"cpf\")\\\n",
    "    .merge(df2, how=\"left\", on=\"cpf\")\\\n",
    "    .fillna(0)\\\n",
    "    .query(\"preco_viagem_1 + preco_viagem_2 < 1000\")\n",
    "\n",
    "dfR = dfR[[\"cpf\", \"nome\", \"preco_viagem_1\", \"preco_viagem_2\"]]\n",
    "\n",
    "dfR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaafbfb2-7956-4348-8cfe-394e5bc92de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Questão 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a568f29-d35f-4e30-a44c-0c5f8b63b850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dataAno</th><th>cargoNivel</th><th>funcSexo</th><th>Média dos Salários</th></tr></thead><tbody><tr><td>2017</td><td>JUNIOR</td><td>F</td><td>2793.01</td></tr><tr><td>2018</td><td>JUNIOR</td><td>F</td><td>2576.66</td></tr><tr><td>2019</td><td>JUNIOR</td><td>F</td><td>2440.23</td></tr><tr><td>2017</td><td>JUNIOR</td><td>M</td><td>2658.8</td></tr><tr><td>2018</td><td>JUNIOR</td><td>M</td><td>2509.45</td></tr><tr><td>2019</td><td>JUNIOR</td><td>M</td><td>2437.86</td></tr><tr><td>2017</td><td>PLENO</td><td>F</td><td>7236.78</td></tr><tr><td>2018</td><td>PLENO</td><td>F</td><td>7947.47</td></tr><tr><td>2019</td><td>PLENO</td><td>F</td><td>7641.94</td></tr><tr><td>2017</td><td>PLENO</td><td>M</td><td>6410.48</td></tr><tr><td>2018</td><td>PLENO</td><td>M</td><td>6260.32</td></tr><tr><td>2019</td><td>PLENO</td><td>M</td><td>6259.61</td></tr><tr><td>2017</td><td>SENIOR</td><td>F</td><td>13253.23</td></tr><tr><td>2018</td><td>SENIOR</td><td>F</td><td>13648.01</td></tr><tr><td>2019</td><td>SENIOR</td><td>F</td><td>12994.19</td></tr><tr><td>2017</td><td>SENIOR</td><td>M</td><td>13974.98</td></tr><tr><td>2018</td><td>SENIOR</td><td>M</td><td>14808.82</td></tr><tr><td>2019</td><td>SENIOR</td><td>M</td><td>14480.5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2017,
         "JUNIOR",
         "F",
         2793.01
        ],
        [
         2018,
         "JUNIOR",
         "F",
         2576.66
        ],
        [
         2019,
         "JUNIOR",
         "F",
         2440.23
        ],
        [
         2017,
         "JUNIOR",
         "M",
         2658.8
        ],
        [
         2018,
         "JUNIOR",
         "M",
         2509.45
        ],
        [
         2019,
         "JUNIOR",
         "M",
         2437.86
        ],
        [
         2017,
         "PLENO",
         "F",
         7236.78
        ],
        [
         2018,
         "PLENO",
         "F",
         7947.47
        ],
        [
         2019,
         "PLENO",
         "F",
         7641.94
        ],
        [
         2017,
         "PLENO",
         "M",
         6410.48
        ],
        [
         2018,
         "PLENO",
         "M",
         6260.32
        ],
        [
         2019,
         "PLENO",
         "M",
         6259.61
        ],
        [
         2017,
         "SENIOR",
         "F",
         13253.23
        ],
        [
         2018,
         "SENIOR",
         "F",
         13648.01
        ],
        [
         2019,
         "SENIOR",
         "F",
         12994.19
        ],
        [
         2017,
         "SENIOR",
         "M",
         13974.98
        ],
        [
         2018,
         "SENIOR",
         "M",
         14808.82
        ],
        [
         2019,
         "SENIOR",
         "M",
         14480.5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dataAno",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cargoNivel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "funcSexo",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Média dos Salários",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "  pagamento\n",
    ".join(data, on=\"dataPK\")\n",
    ".join(cargo, on=\"cargoPK\")\n",
    ".join(funcionario, on=\"funcPK\")\n",
    ".where(\"dataAno >= 2017 and dataAno <= 2019\")\n",
    ".select(\"dataAno\", \"cargoNivel\", \"funcSexo\", \"salario\")\n",
    ".groupBy(\"dataAno\", \"cargoNivel\", \"funcSexo\").avg(\"salario\")\n",
    ".orderBy(\"cargoNivel\",\"funcSexo\")\n",
    ".withColumn(\"avg(salario)\", f.round(\"avg(salario)\",2))\n",
    ".withColumnRenamed(\"avg(salario)\", \"Média dos Salários\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b009c77-f90e-4e82-8c55-2b25cf4fe25a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vqsvtdy4CdVw"
   },
   "source": [
    "Lembre-se que é possível especificar as consultas analíticas usando Pandas, o método spark.sql() ou os métodos do módulo pyspark.sql."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac3eedc2-e029-4b3a-bd9f-9ce3374b4dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZQcs4bV9uRf-"
   },
   "source": [
    "## Questão 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "880dcfea-65c6-418f-bbcc-12e7ad07b9d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Considere a constelação de fatos da BI Solutions e a seguinte solicitação de consulta: “Liste todas as agregações que podem ser geradas a partir da média dos salários dos funcionários que moram na região NORDESTE por sexo e por ano de nascimento”. Arredonde a média dos salários para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: SEXO, ANO e MEDIASALARIO. Ordene as linhas exibidas primeiro por sexo, depois por ano de nascimento, depois por média dos salários, todos em ordem ascendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aba24d4-ffe1-4954-a980-a601cb2c74e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>SEXO</th><th>ANO</th><th>MEDIASALARIO</th></tr></thead><tbody><tr><td>null</td><td>null</td><td>7151.73</td></tr><tr><td>null</td><td>1951</td><td>4116.83</td></tr><tr><td>null</td><td>1952</td><td>3635.55</td></tr><tr><td>null</td><td>1953</td><td>11842.55</td></tr><tr><td>null</td><td>1965</td><td>1919.34</td></tr><tr><td>null</td><td>1966</td><td>14995.02</td></tr><tr><td>null</td><td>1967</td><td>6686.45</td></tr><tr><td>null</td><td>1978</td><td>2581.24</td></tr><tr><td>null</td><td>1979</td><td>6300.52</td></tr><tr><td>null</td><td>1980</td><td>2085.85</td></tr><tr><td>null</td><td>1990</td><td>7782.07</td></tr><tr><td>F</td><td>null</td><td>7043.43</td></tr><tr><td>F</td><td>1953</td><td>11842.55</td></tr><tr><td>F</td><td>1990</td><td>5935.93</td></tr><tr><td>M</td><td>null</td><td>7183.82</td></tr><tr><td>M</td><td>1951</td><td>4116.83</td></tr><tr><td>M</td><td>1952</td><td>3635.55</td></tr><tr><td>M</td><td>1965</td><td>1919.34</td></tr><tr><td>M</td><td>1966</td><td>14995.02</td></tr><tr><td>M</td><td>1967</td><td>6686.45</td></tr><tr><td>M</td><td>1978</td><td>2581.24</td></tr><tr><td>M</td><td>1979</td><td>6300.52</td></tr><tr><td>M</td><td>1980</td><td>2085.85</td></tr><tr><td>M</td><td>1990</td><td>8742.07</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         null,
         7151.73
        ],
        [
         null,
         1951,
         4116.83
        ],
        [
         null,
         1952,
         3635.55
        ],
        [
         null,
         1953,
         11842.55
        ],
        [
         null,
         1965,
         1919.34
        ],
        [
         null,
         1966,
         14995.02
        ],
        [
         null,
         1967,
         6686.45
        ],
        [
         null,
         1978,
         2581.24
        ],
        [
         null,
         1979,
         6300.52
        ],
        [
         null,
         1980,
         2085.85
        ],
        [
         null,
         1990,
         7782.07
        ],
        [
         "F",
         null,
         7043.43
        ],
        [
         "F",
         1953,
         11842.55
        ],
        [
         "F",
         1990,
         5935.93
        ],
        [
         "M",
         null,
         7183.82
        ],
        [
         "M",
         1951,
         4116.83
        ],
        [
         "M",
         1952,
         3635.55
        ],
        [
         "M",
         1965,
         1919.34
        ],
        [
         "M",
         1966,
         14995.02
        ],
        [
         "M",
         1967,
         6686.45
        ],
        [
         "M",
         1978,
         2581.24
        ],
        [
         "M",
         1979,
         6300.52
        ],
        [
         "M",
         1980,
         2085.85
        ],
        [
         "M",
         1990,
         8742.07
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "SEXO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ANO",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "MEDIASALARIO",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "  pagamento\n",
    "  .join(funcionario, on='funcPK', how='full')\n",
    "  .filter(f.col('funcRegiaoNome') == 'NORDESTE')\n",
    "  .cube(['funcAnoNascimento', 'funcSexo'])\n",
    "  .agg(f.round(f.mean('salario'), 2).alias('MEDIASALARIO'))\n",
    "  .orderBy(['funcSexo', 'funcAnoNascimento', 'MEDIASALARIO'], ascending=True)\n",
    "  .select(f.col('funcSexo').alias('SEXO'), f.col('funcAnoNascimento').alias('ANO'), 'MEDIASALARIO')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed807c4-0798-47a5-9f00-cfacb9859c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CDzO2MJouUWD"
   },
   "outputs": [],
   "source": [
    "# Resposta da Questão 7\n",
    "# Não se esqueça de comentar detalhadamente a sua solução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc36d075-08f5-43cb-89e0-7726165115f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "bcSouuCk6N0-"
   },
   "source": [
    "## Questão 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d80b6fbe-8eba-43eb-8e83-53fe32efa82b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ANO</th><th>SETOR</th><th>TOTALRECEITA</th></tr></thead><tbody><tr><td>2019</td><td>BEBIDAS E ALIMENTOS</td><td>3995904.25</td></tr><tr><td>2019</td><td>TECNOLOGIA</td><td>5778946.3</td></tr><tr><td>2020</td><td>BEBIDAS E ALIMENTOS</td><td>4264757.85</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2019,
         "BEBIDAS E ALIMENTOS",
         3995904.25
        ],
        [
         2019,
         "TECNOLOGIA",
         5778946.3
        ],
        [
         2020,
         "BEBIDAS E ALIMENTOS",
         4264757.85
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ANO",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "SETOR",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TOTALRECEITA",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "  cliente\n",
    "  .join(negociacao, on='clientePK', how='inner')\n",
    "  .join(data, on='dataPK', how='inner')\n",
    "  .filter(\n",
    "    ((f.col('dataAno') == 2019) | ((f.col('dataAno') == 2020)))\n",
    "    & (f.col('clienteEstadoNome') == 'SAO PAULO')\n",
    "    )\n",
    "  .groupBy('clienteSetor', 'dataAno')\n",
    "  .agg(f.round(f.sum('receita'), 2).alias('TOTALRECEITA'))\n",
    "  .select(f.col(\"dataAno\").alias('ANO'), f.col('clienteSetor').alias('SETOR'), 'TOTALRECEITA')\n",
    "  .filter(f.col('TOTALRECEITA') > 3900000.00)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c506184-cd10-4e20-afa3-2ce5cedc8b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4OjKqHvginUV"
   },
   "outputs": [],
   "source": [
    "# Resposta da Questão 8\n",
    "# Não se esqueça de comentar detalhadamente a sua solução\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e52155d2-7379-4fbe-af68-8311e0c67a77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fpFnRTk369Q5"
   },
   "source": [
    "## Questão 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5b2e01-bcca-4c7d-ad6c-8fa83d43078c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EQUIPENOME</th><th>EQUIPEFILIAL</th><th>RECEITA2019</th><th>RECEITA2020</th><th>DIFERENCA</th></tr></thead><tbody><tr><td>BI & ANALYTICS</td><td>RECIFE - CENTRO</td><td>1.23106465E7</td><td>8791572.9</td><td>-3519074.0</td></tr><tr><td>BI & ANALYTICS</td><td>SAO PAULO - AV. PAULISTA</td><td>1.12672264E7</td><td>1.073077355E7</td><td>-536453.0</td></tr><tr><td>APP - DESKTOP</td><td>RIO DE JANEIRO - BARRA DA TIJUCA</td><td>2417619.9</td><td>2290441.3</td><td>-127179.0</td></tr><tr><td>APP - DESKTOP</td><td>SAO PAULO - AV. PAULISTA</td><td>2009714.0</td><td>2146181.25</td><td>136467.0</td></tr><tr><td>APP - MOBILE</td><td>SAO PAULO - AV. PAULISTA</td><td>1779690.6</td><td>1243670.55</td><td>-536020.0</td></tr><tr><td>APP - MOBILE</td><td>RIO DE JANEIRO - BARRA DA TIJUCA</td><td>1645371.0</td><td>1289305.0</td><td>-356066.0</td></tr><tr><td>APP - MOBILE</td><td>CAMPO GRANDE - CENTRO</td><td>1280914.9</td><td>1347929.7</td><td>67015.0</td></tr><tr><td>WEB</td><td>RIO DE JANEIRO - BARRA DA TIJUCA</td><td>1037993.7</td><td>612673.9</td><td>-425320.0</td></tr><tr><td>WEB</td><td>CAMPO GRANDE - CENTRO</td><td>956287.3</td><td>1017644.05</td><td>61357.0</td></tr><tr><td>WEB</td><td>SAO PAULO - AV. PAULISTA</td><td>647854.0</td><td>751983.75</td><td>104130.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "BI & ANALYTICS",
         "RECIFE - CENTRO",
         12310646.5,
         8791572.9,
         -3519074
        ],
        [
         "BI & ANALYTICS",
         "SAO PAULO - AV. PAULISTA",
         11267226.4,
         10730773.55,
         -536453
        ],
        [
         "APP - DESKTOP",
         "RIO DE JANEIRO - BARRA DA TIJUCA",
         2417619.9,
         2290441.3,
         -127179
        ],
        [
         "APP - DESKTOP",
         "SAO PAULO - AV. PAULISTA",
         2009714,
         2146181.25,
         136467
        ],
        [
         "APP - MOBILE",
         "SAO PAULO - AV. PAULISTA",
         1779690.6,
         1243670.55,
         -536020
        ],
        [
         "APP - MOBILE",
         "RIO DE JANEIRO - BARRA DA TIJUCA",
         1645371,
         1289305,
         -356066
        ],
        [
         "APP - MOBILE",
         "CAMPO GRANDE - CENTRO",
         1280914.9,
         1347929.7,
         67015
        ],
        [
         "WEB",
         "RIO DE JANEIRO - BARRA DA TIJUCA",
         1037993.7,
         612673.9,
         -425320
        ],
        [
         "WEB",
         "CAMPO GRANDE - CENTRO",
         956287.3,
         1017644.05,
         61357
        ],
        [
         "WEB",
         "SAO PAULO - AV. PAULISTA",
         647854,
         751983.75,
         104130
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EQUIPENOME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EQUIPEFILIAL",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "RECEITA2019",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "RECEITA2020",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "DIFERENCA",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "  equipe\n",
    "  .join(negociacao, on='equipePK', how='inner')\n",
    "  .join(data, on='dataPK', how='inner')\n",
    "  .withColumn(\n",
    "    \"RECEITA2019\", f.when(\n",
    "       f.col('dataAno') == 2019, f.col('receita')).otherwise(0)\n",
    "      )\n",
    "  .withColumn(\n",
    "    \"RECEITA2020\", f.when(\n",
    "       f.col('dataAno') == 2020, f.col('receita')).otherwise(0)\n",
    "      )\n",
    "  .withColumn(\n",
    "    \"DIFERENCA\", f.col('RECEITA2020') - f.col('RECEITA2019')\n",
    "  )\n",
    "  .filter(\n",
    "    ((f.col('dataAno') == 2019) | ((f.col('dataAno') == 2020)))\n",
    "    )\n",
    "  .groupBy('equipeNome', 'filialNome')\n",
    "  .agg( \n",
    "       f.round(f.sum(\"RECEITA2019\"), 2).alias(\"RECEITA2019\")\n",
    "       , f.round(f.sum(\"RECEITA2020\"), 2).alias(\"RECEITA2020\")\n",
    "       , f.round(f.sum(\"DIFERENCA\")).alias(\"DIFERENCA\") )\n",
    "  .select(\n",
    "    f.col('equipeNome').alias(\"EQUIPENOME\")\n",
    "    , f.col(\"filialNome\").alias(\"EQUIPEFILIAL\")\n",
    "    , \"RECEITA2019\", \"RECEITA2020\", \"DIFERENCA\"\n",
    "  )\n",
    " #.orderBy('RECEITA2019', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e0b88ee-01f0-43ca-844d-c0bc38e1db5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "m98NQbOiio-I"
   },
   "outputs": [],
   "source": [
    "# Resposta da Questão 9\n",
    "# Não se esqueça de comentar detalhadamente a sua solução\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b75f014-2b05-40d1-9948-cdb9e41f4d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zCpy46lL8Xdn"
   },
   "source": [
    "## Questão 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6edcf03a-7185-4e44-85e8-09f554cc1714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ESTADO</th><th>REGIAO</th><th>TOTALSALARIOS</th></tr></thead><tbody><tr><td>SAO PAULO</td><td>SUDESTE</td><td>4623213.72</td></tr><tr><td>MINAS GERAIS</td><td>SUDESTE</td><td>2133676.2</td></tr><tr><td>PARANA</td><td>SUL</td><td>2006090.28</td></tr><tr><td>RIO DE JANEIRO</td><td>SUDESTE</td><td>1094470.32</td></tr><tr><td>PERNAMBUCO</td><td>NORDESTE</td><td>822273.24</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "SAO PAULO",
         "SUDESTE",
         4623213.72
        ],
        [
         "MINAS GERAIS",
         "SUDESTE",
         2133676.2
        ],
        [
         "PARANA",
         "SUL",
         2006090.28
        ],
        [
         "RIO DE JANEIRO",
         "SUDESTE",
         1094470.32
        ],
        [
         "PERNAMBUCO",
         "NORDESTE",
         822273.24
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ESTADO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "REGIAO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TOTALSALARIOS",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "  funcionario\n",
    "  .join(pagamento, on='funcPK', how='inner')\n",
    "  .join(data, on='dataPK', how='inner')\n",
    "  .join(cargo, on='cargoPK', how='inner')\n",
    "  .filter(\n",
    "    (f.col('dataAno') == 2020)\n",
    "    & (f.col('cargoNivel') == 'SENIOR')\n",
    "  )\n",
    "  .groupBy('funcEstadoNome', 'funcRegiaoNome')\n",
    "  .agg(f.round(f.sum('salario'), 2).alias('TOTALSALARIOS'))\n",
    "  .select(\n",
    "    f.col(\"funcEstadoNome\").alias('ESTADO'), f.col('funcRegiaoNome').alias('REGIAO'), 'TOTALSALARIOS')\n",
    "  .orderBy('TOTALSALARIOS', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49744098-c74f-4bfd-b2fc-3012e5e0a09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[87]: 1395067.1999999993"
     ]
    }
   ],
   "source": [
    "4623213.72-(2133676.2+1094470.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d665d7-35c8-4b3f-8635-df807ae7939a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[89]: -694687.3199999998"
     ]
    }
   ],
   "source": [
    "2133676.2-(2006090.28+822273.24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b9fb931-adaf-40ec-8273-73d36876b835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dK3Ln6g_iqR_"
   },
   "outputs": [],
   "source": [
    "# Resposta da Questão 10\n",
    "# Não se esqueça de comentar detalhadamente a sua solução\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Avaliação Final 2024 Notebook",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
